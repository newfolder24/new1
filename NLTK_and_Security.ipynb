{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-vBtV8j6n1G",
        "outputId": "746b6fe0-6263-4e44-9342-901a635ba889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: Your sample text here.\n",
            "Tokenized words: ['your', 'sample', 'text', 'here', '.']\n",
            "Filtered words: ['sample', 'text', '.']\n",
            "Stemmed words: ['sampl', 'text', '.']\n",
            "Lemmatized words: ['sample', 'text', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Make sure punkt and stopwords are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize text (split into words)\n",
        "    print(\"Original text:\", text)  # Debug: Check input text\n",
        "    words = word_tokenize(text.lower())  # Convert text to lowercase\n",
        "    print(\"Tokenized words:\", words)  # Debug: Check tokenized words\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    print(\"Filtered words:\", filtered_words)  # Debug: Check filtered words\n",
        "\n",
        "    # Stem words\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "    print(\"Stemmed words:\", stemmed_words)  # Debug: Check stemmed words\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "    print(\"Lemmatized words:\", lemmatized_words)  # Debug: Check lemmatized words\n",
        "\n",
        "    return stemmed_words, lemmatized_words  # Return both stemmed and lemmatized words\n",
        "\n",
        "# Example usage:\n",
        "text = \"Your sample text here.\"\n",
        "stemmed, lemmatized = preprocess(text)\n",
        "#print(\"Stemmed:\", stemmed)\n",
        "#print(\"Lemmatized:\", lemmatized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#or for nltk"
      ],
      "metadata": {
        "id": "xHT-GrCUcX4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"A Natural Language Processing enables computers to analyze and understand human language.\"\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "def stem_text(text):\n",
        "    text_no_stopwords = remove_stop_words(text)\n",
        "    snowball_stemmer = SnowballStemmer(\"english\")\n",
        "    words = text_no_stopwords.split()\n",
        "    return ' '.join([snowball_stemmer.stem(word) for word in words])\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text_no_stopwords = remove_stop_words(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text_no_stopwords.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "def switch(ch):\n",
        "    if ch == 1:\n",
        "        return tokenize_text(text)\n",
        "    elif ch == 2:\n",
        "        return remove_stop_words(text)\n",
        "    elif ch == 3:\n",
        "        return stem_text(text)\n",
        "    elif ch == 4:\n",
        "        return lemmatize_text(text)\n",
        "    else:\n",
        "        return \"Invalid choice. Please enter a number between 1 and 4.\"\n",
        "\n",
        "print(\"Choose a preprocessing method by entering the corresponding number:\")\n",
        "print(\"1 - Tokenization\")\n",
        "print(\"2 - Stop Words Removal\")\n",
        "print(\"3 - Stemming (after removing stopwords)\")\n",
        "print(\"4 - Lemmatization (after removing stopwords)\")\n",
        "ch = int(input(\"Enter your choice: \"))\n",
        "\n",
        "# Display result based on user input\n",
        "result = switch(ch)\n",
        "print(\"\\nOriginal Text:\\n\", text)\n",
        "print(\"\\nProcessed Text:\\n\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdTFaXqHawyO",
        "outputId": "e5a89914-6055-4e25-e558-b65518806832"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose a preprocessing method by entering the corresponding number:\n",
            "1 - Tokenization\n",
            "2 - Stop Words Removal\n",
            "3 - Stemming (after removing stopwords)\n",
            "4 - Lemmatization (after removing stopwords)\n",
            "Enter your choice: 3\n",
            "\n",
            "Original Text:\n",
            " A Natural Language Processing enables computers to analyze and understand human language.\n",
            "\n",
            "Processed Text:\n",
            " natur languag process enabl comput analyz understand human language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample random text (100 words)\n",
        "random_text = \"\"\"\n",
        "Data processing encompasses a series of operations that convert raw data into structured\n",
        "and organized information. This process begins with data collection, where data is gathered\n",
        "from various sources such as sensors, databases, forms, or external systems. Once collected,\n",
        "the data can be in various formats, including text, numbers, images, or multimedia.\n",
        "The next step in data processing is data cleaning and validation. This involves identifying\n",
        "and correcting errors, inconsistencies, and missing values in the data. Clean and accurate data\n",
        "is essential for reliable analysis and decision-making. Data cleaning often involves techniques\n",
        "like outlier detection and data imputation.\n",
        "After data cleaning, data transformation is performed. This includes tasks like data normalization,\n",
        "aggregation, and summarization. Normalization ensures that data is on a consistent scale, while\n",
        "aggregation and summarization reduce data complexity by generating statistics or aggregating data into meaningful groups.\n",
        "Data processing also includes data integration, where data from multiple sources is combined\n",
        "into a unified dataset. Integration can be challenging due to differences in data structures and\n",
        "formats. Techniques like data mapping and data warehousing are used to facilitate integration.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(random_text)\n",
        "\n",
        "# Initialize the NLTK Porter Stemmer and WordNet Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Get the English stop words\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Initialize a list to store the preprocessed words\n",
        "preprocessed_words = []\n",
        "\n",
        "# Perform text preprocessing\n",
        "for word in words:\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    word = word.lower()\n",
        "    word = word.strip('.,?!-()[]{}\"\\'')\n",
        "\n",
        "    # Check if the word is not a stop word\n",
        "    if word not in stop_words:\n",
        "        # Stem the word\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "\n",
        "        # Lemmatize the stemmed word\n",
        "        lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n",
        "\n",
        "        # Add the lemmatized word to the list\n",
        "        preprocessed_words.append(lemmatized_word)\n",
        "\n",
        "# Join the preprocessed words back into a text\n",
        "preprocessed_text = \" \".join(preprocessed_words)\n",
        "\n",
        "# Print the original text and preprocessed text\n",
        "print(\"Original Text:\")\n",
        "print(random_text)\n",
        "print(\"\\nPreprocessed Text:\")\n",
        "print(preprocessed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2FkXG5Uc9l3",
        "outputId": "5d1bc85b-2c76-40ad-e300-402111aa15a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "Data processing encompasses a series of operations that convert raw data into structured\n",
            "and organized information. This process begins with data collection, where data is gathered\n",
            "from various sources such as sensors, databases, forms, or external systems. Once collected,\n",
            "the data can be in various formats, including text, numbers, images, or multimedia.\n",
            "The next step in data processing is data cleaning and validation. This involves identifying\n",
            "and correcting errors, inconsistencies, and missing values in the data. Clean and accurate data\n",
            "is essential for reliable analysis and decision-making. Data cleaning often involves techniques\n",
            "like outlier detection and data imputation.\n",
            "After data cleaning, data transformation is performed. This includes tasks like data normalization,\n",
            "aggregation, and summarization. Normalization ensures that data is on a consistent scale, while\n",
            "aggregation and summarization reduce data complexity by generating statistics or aggregating data into meaningful groups.\n",
            "Data processing also includes data integration, where data from multiple sources is combined\n",
            "into a unified dataset. Integration can be challenging due to differences in data structures and\n",
            "formats. Techniques like data mapping and data warehousing are used to facilitate integration.\n",
            "\n",
            "\n",
            "Preprocessed Text:\n",
            "data process encompass seri oper convert raw data structur organ inform  process begin data collect  data gather variou sourc sensor  databas  form  extern system  collect  data variou format  includ text  number  imag  multimedia  next step data process data clean valid  involv identifi correct error  inconsist  miss valu data  clean accur data essenti reliabl analysi decision-mak  data clean often involv techniqu like outlier detect data imput  data clean  data transform perform  includ task like data normal  aggreg  summar  normal ensur data consist scale  aggreg summar reduc data complex gener statist aggreg data meaning group  data process also includ data integr  data multipl sourc combin unifi dataset  integr challeng due differ data structur format  techniqu like data map data wareh use facilit integr \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#security code"
      ],
      "metadata": {
        "id": "LpE3tCt0cPBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib  # For hashing to ensure data integrity\n",
        "from cryptography.fernet import Fernet  # For encryption\n",
        "import random  # For simulating sensor data"
      ],
      "metadata": {
        "id": "skFwm5gR6tIm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an encryption key\n",
        "key = Fernet.generate_key()\n",
        "cipher = Fernet(key)"
      ],
      "metadata": {
        "id": "V7BocP1NYG6j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sensor_data():\n",
        "    return random.uniform(50, 100)  # Random temperature between 50°C and 100°C\n",
        "\n",
        "# Encrypt the sensor data\n",
        "def encrypt_data(data, cipher):\n",
        "    return cipher.encrypt(data.encode())\n",
        "\n",
        "# Decrypt the sensor data\n",
        "def decrypt_data(encrypted_data, cipher):\n",
        "    return cipher.decrypt(encrypted_data).decode()\n",
        "\n",
        "# Verify data integrity using hashing\n",
        "def verify_data_integrity(data):\n",
        "    return hashlib.sha256(data.encode()).hexdigest()"
      ],
      "metadata": {
        "id": "RdxcBYAoYJNo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensor_data = f\"Temperature: {get_sensor_data():.2f}°C\"\n",
        "print(\"Original Data:\", sensor_data)\n",
        "\n",
        "# Encrypt and then decrypt the data\n",
        "encrypted_data = encrypt_data(sensor_data, cipher)\n",
        "print(\"Encrypted Data:\", encrypted_data)\n",
        "\n",
        "decrypted_data = decrypt_data(encrypted_data, cipher)\n",
        "print(\"Decrypted Data:\", decrypted_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz2BAE75YMef",
        "outputId": "582c9edc-4e6e-4d36-83f0-1180728bb8e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data: Temperature: 96.15°C\n",
            "Encrypted Data: b'gAAAAABnK53w3ew634WnyKtg7nTvBBO55us2kTmSLZDOm4AZoGFN2w8i1SX8gJ9V2MgBXNdqoK0dZKG0xuU7WNqtLBUUsCEwwRaegA1QR02FadabwjwEEak='\n",
            "Decrypted Data: Temperature: 96.15°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_hash = verify_data_integrity(sensor_data)\n",
        "decrypted_hash = verify_data_integrity(decrypted_data)\n",
        "if original_hash == decrypted_hash:\n",
        "    print(\"Data Integrity Verified: Hashes match.\")\n",
        "else:\n",
        "    print(\"Data Integrity Issue: Hashes do not match.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B8Vp0Q9YOqc",
        "outputId": "4a9597df-f4c4-4a46-c09c-d9c1c7b0d3f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Integrity Verified: Hashes match.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQpBH5C0YQgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}